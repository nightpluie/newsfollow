# Newsfollow è¨­è¨ˆå¯©æŸ¥å ±å‘Š

## å°ˆæ¡ˆç›®æ¨™å›é¡§

ç›£æ§å°ç£å¤šå®¶åª’é«”ç¶²ç«™,è‡ªå‹•åµæ¸¬é‡å¤§æ–°èäº‹ä»¶,ä¸¦ç”¢ç”Ÿ ETtoday ç™¼å¸ƒç”¨è‰ç¨¿ã€‚

**ç•¶å‰å¯¦ä½œ:** UDN + TVBS (å¯¦é©—éšæ®µ)
**æ“´å±•ç›®æ¨™:** æ”¯æ´ 10+ å®¶åª’é«”

---

## ğŸ”´ é—œéµè¨­è¨ˆç¼ºé™·

### 1. ç¨‹å¼ç¢¼çµ„ç¹”å•é¡Œ

**å•é¡Œ:** `main.py` æœ‰ 1133 è¡Œ,é•å 800 è¡Œä¸Šé™è¦ç¯„

**å½±éŸ¿:**
- é›£ä»¥ç¶­è­·å’Œæ¸¬è©¦
- æ–°å¢åª’é«”ä¾†æºæ™‚ä¿®æ”¹é¢¨éšªé«˜
- é•åå–®ä¸€è·è²¬åŸå‰‡

**å»ºè­°é‡æ§‹:**
```
newsfollow/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ models.py          # Signal, Event ç­‰è³‡æ–™æ¨¡å‹
â”‚   â””â”€â”€ config.py          # è¨­å®šè¼‰å…¥èˆ‡é©—è­‰
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ base.py            # çˆ¬èŸ²åŸºåº•é¡åˆ¥
â”‚   â”œâ”€â”€ requests_backend.py
â”‚   â””â”€â”€ openclaw_backend.py
â”œâ”€â”€ detection/
â”‚   â”œâ”€â”€ clustering.py      # äº‹ä»¶èšé¡æ¼”ç®—æ³•
â”‚   â””â”€â”€ scoring.py         # è©•åˆ†æ©Ÿåˆ¶
â”œâ”€â”€ generation/
â”‚   â””â”€â”€ draft_generator.py # LLM è‰ç¨¿ç”Ÿæˆ
â”œâ”€â”€ publisher/
â”‚   â””â”€â”€ adapters.py        # ç™¼å¸ƒä»‹é¢å¡
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ repository.py      # è³‡æ–™åº«æ“ä½œ
â””â”€â”€ main.py                # ä¸»ç¨‹å¼é€²å…¥é» (<200 è¡Œ)
```

### 2. æ¸¬è©¦è¦†è“‹ç‡ç‚ºé›¶

**å•é¡Œ:** å®Œå…¨æ²’æœ‰æ¸¬è©¦,é•å 80% è¦†è“‹ç‡è¦æ±‚

**é¢¨éšª:**
- ç„¡æ³•ç¢ºä¿ç¨‹å¼æ­£ç¢ºæ€§
- é‡æ§‹æ™‚å®¹æ˜“å¼•å…¥ bug
- ä¸Šç·šå¾Œé›£ä»¥é™¤éŒ¯

**å¿…é ˆåŠ å…¥çš„æ¸¬è©¦:**
```python
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_clustering.py      # èšé¡æ¼”ç®—æ³•å–®å…ƒæ¸¬è©¦
â”‚   â”œâ”€â”€ test_scoring.py         # è©•åˆ†é‚è¼¯å–®å…ƒæ¸¬è©¦
â”‚   â””â”€â”€ test_title_similarity.py
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_crawler.py         # çˆ¬èŸ²æ•´åˆæ¸¬è©¦
â”‚   â”œâ”€â”€ test_repository.py     # è³‡æ–™åº«æ•´åˆæ¸¬è©¦
â”‚   â””â”€â”€ test_llm_generation.py # LLM æ•´åˆæ¸¬è©¦
â””â”€â”€ e2e/
    â””â”€â”€ test_workflow.py        # ç«¯åˆ°ç«¯æ¸¬è©¦
```

### 3. CSS é¸æ“‡å™¨è„†å¼±æ€§ (æœ€åš´é‡)

**å•é¡Œ:** åª’é«”ç¶²ç«™æ”¹ç‰ˆæœƒå°è‡´çˆ¬èŸ²ç«‹å³å¤±æ•ˆ

**ç•¶å‰å¯¦ä½œ:**
```yaml
selectors:
  - "a.story-list__title-link"  # é«˜åº¦ä¾è³´ class name
  - ".breaking-news a"
```

**å¤±æ•ˆå ´æ™¯:**
- UDN/TVBS ç¶²ç«™æ”¹ç‰ˆ
- CSS class åç¨±è®Šæ›´
- HTML çµæ§‹èª¿æ•´

**æ”¹é€²æ–¹æ¡ˆ:**

**æ–¹æ¡ˆ A: å¤šå±¤ Fallback**
```yaml
selectors:
  primary:
    - "a.story-list__title-link"
  secondary:
    - ".story-list a"
    - "article a[href*='/news/']"
  generic:
    - "main a[href]"
```

**æ–¹æ¡ˆ B: å¥åº·åº¦ç›£æ§**
```python
class SelectorHealthCheck:
    def check(self, url, selectors):
        """æª¢æŸ¥é¸æ“‡å™¨æ˜¯å¦ä»æœ‰æ•ˆ"""
        results = fetch_with_selectors(url, selectors)

        if len(results) < 5:
            alert("é¸æ“‡å™¨å¯èƒ½å¤±æ•ˆ: {url}")
            # è‡ªå‹•å˜—è©¦ generic selectors
            fallback_results = fetch_with_generic(url)
            return fallback_results

        return results
```

**æ–¹æ¡ˆ C: è‡ªé©æ‡‰é¸æ“‡å™¨ (é€²éš)**
- ä½¿ç”¨ AI åˆ†æé é¢çµæ§‹
- è‡ªå‹•å­¸ç¿’æ–°çš„é¸æ“‡å™¨æ¨¡å¼
- éœ€è¦è¼ƒé«˜é–‹ç™¼æˆæœ¬

### 4. åŒæ­¥çˆ¬èŸ²æ•ˆèƒ½ç“¶é ¸

**å•é¡Œ:** çˆ¬èŸ²æ˜¯åŒæ­¥åŸ·è¡Œ,æ“´å±•åˆ° 10+ åª’é«”æ™‚æœƒå¾ˆæ…¢

**ç•¶å‰æ•ˆèƒ½:**
- 2 å€‹åª’é«” Ã— 3 å€‹ section = 6 æ¬¡è«‹æ±‚
- æ¯æ¬¡è«‹æ±‚ 2-5 ç§’
- **ç¸½æ™‚é–“: 12-30 ç§’**

**æ“´å±•å¾Œæ•ˆèƒ½:**
- 10 å€‹åª’é«” Ã— 3 å€‹ section = 30 æ¬¡è«‹æ±‚
- **ç¸½æ™‚é–“: 60-150 ç§’** âŒ ä¸å¯æ¥å—

**è§£æ±ºæ–¹æ¡ˆ:**

```python
import asyncio
import aiohttp

async def fetch_all_sources(sources):
    """ä¸¦è¡Œçˆ¬å–æ‰€æœ‰ä¾†æº"""
    async with aiohttp.ClientSession() as session:
        tasks = []
        for source in sources:
            for section in source['sections']:
                tasks.append(fetch_section(session, section))

        results = await asyncio.gather(*tasks)
        return results

# æ•ˆèƒ½æå‡:
# 30 æ¬¡è«‹æ±‚ Ã— 3 ç§’ / è«‹æ±‚ = 90 ç§’ (åŒæ­¥)
# max(30 æ¬¡è«‹æ±‚) â‰ˆ 5 ç§’ (ä¸¦è¡Œ) âœ…
```

### 5. ç¼ºå°‘éŒ¯èª¤ç›£æ§èˆ‡å‘Šè­¦

**å•é¡Œ:** çˆ¬èŸ²å¤±æ•—æ™‚åªè¨˜éŒ„ log,ä½¿ç”¨è€…ä¸çŸ¥é“ç³»çµ±æ˜¯å¦æ­£å¸¸

**é¢¨éšªå ´æ™¯:**
- é¸æ“‡å™¨å¤±æ•ˆ â†’ ç„¡è³‡æ–™ â†’ æ²’äººç™¼ç¾
- ç¶²ç«™å°é– â†’ çˆ¬èŸ²å¤±æ•— â†’ ç¹¼çºŒé‹ä½œä½†æ²’è¼¸å‡º
- LLM API å¤±æ•— â†’ fallback æ¨¡å¼ â†’ è‰ç¨¿å“è³ªä¸‹é™

**å¿…è¦ç›£æ§æŒ‡æ¨™:**
```python
class Metrics:
    # çˆ¬èŸ²å¥åº·åº¦
    crawler_success_rate: float  # æ‡‰ > 90%
    signals_per_run: int         # æ‡‰ > 50

    # äº‹ä»¶åµæ¸¬
    events_per_run: int          # æ­£å¸¸ç¯„åœ 0-10
    avg_event_score: float       # æ‡‰ > 12

    # LLM ç”Ÿæˆ
    llm_success_rate: float      # æ‡‰ > 95%
    fallback_ratio: float        # æ‡‰ < 5%

    def alert_if_abnormal(self):
        """ç•°å¸¸æ™‚ç™¼é€å‘Šè­¦"""
        if self.crawler_success_rate < 0.9:
            send_alert("çˆ¬èŸ²æˆåŠŸç‡éä½")

        if self.signals_per_run < 20:
            send_alert("æ¡é›†åˆ°çš„æ–°èæ•¸é‡ç•°å¸¸")
```

### 6. SQLite æ“´å±•æ€§é™åˆ¶

**å•é¡Œ:** SQLite å–®æª”æ¡ˆ,ä¸é©åˆé«˜é »å¯«å…¥å ´æ™¯

**é™åˆ¶:**
- å¯«å…¥æ™‚é–è¡¨
- ç„¡æ³•æ°´å¹³æ“´å±•
- ä½µç™¼æ•ˆèƒ½å·®

**ä½•æ™‚éœ€è¦å‡ç´š:**
- ç›£æ§é »ç‡ < 60 ç§’
- ç›£æ§åª’é«” > 20 å®¶
- éœ€è¦å¤šæ©Ÿéƒ¨ç½²
- éœ€è¦æ­·å²è³‡æ–™åˆ†æ

**å»ºè­°:**
- **çŸ­æœŸ:** ç¹¼çºŒä½¿ç”¨ SQLite (å¯¦é©—éšæ®µè¶³å¤ )
- **ä¸­æœŸ:** å‡ç´šåˆ° PostgreSQL
- **é•·æœŸ:** è€ƒæ…® TimescaleDB (æ™‚åºè³‡æ–™)

### 7. ç¼ºå°‘ Rate Limiting

**å•é¡Œ:** å¯èƒ½è¢«åª’é«”ç¶²ç«™å°é–

**è§£æ±ºæ–¹æ¡ˆ:**
```python
from ratelimit import limits, sleep_and_retry

class RateLimitedCrawler:
    @sleep_and_retry
    @limits(calls=10, period=60)  # æ¯åˆ†é˜æœ€å¤š 10 æ¬¡è«‹æ±‚
    def fetch(self, url):
        return requests.get(url)
```

### 8. LLM æˆæœ¬æ§åˆ¶

**å•é¡Œ:** æ¯å€‹äº‹ä»¶éƒ½å‘¼å« LLM,æˆæœ¬é«˜

**æˆæœ¬ä¼°ç®—:**
- æ¯æ¬¡å‘¼å« ~1000 tokens
- gpt-4o-mini: $0.15 / 1M tokens
- æ¯å°æ™‚ 10 å€‹äº‹ä»¶ Ã— 24 å°æ™‚ = 240 æ¬¡å‘¼å«/å¤©
- **æˆæœ¬: $0.036/å¤©** (å¯æ¥å—)

**ä½†å¦‚æœæ“´å±•åˆ° 20+ åª’é«”:**
- æ¯å°æ™‚ 50 å€‹äº‹ä»¶ Ã— 24 å°æ™‚ = 1200 æ¬¡å‘¼å«/å¤©
- **æˆæœ¬: $0.18/å¤© = $5.4/æœˆ**

**å„ªåŒ–æ–¹æ¡ˆ:**
```python
# 1. å¿«å–ç›¸åŒäº‹ä»¶
if event_exists_in_cache(event_key):
    return get_cached_draft(event_key)

# 2. åªå°é«˜åˆ†äº‹ä»¶ç”Ÿæˆ LLM è‰ç¨¿
if event.score < 15:
    return fallback_draft(event)

# 3. æ‰¹æ¬¡ç”Ÿæˆ (é™ä½ API å‘¼å«æ¬¡æ•¸)
drafts = batch_generate([event1, event2, event3])
```

---

## âš ï¸ ä¸­ç­‰å„ªå…ˆç´šå•é¡Œ

### 9. ç¼ºå°‘è¼¸å…¥é©—è­‰

**å•é¡Œ:** config.yaml æ²’æœ‰ schema é©—è­‰

**æ”¹é€²:**
```python
from pydantic import BaseModel, HttpUrl, validator

class SectionConfig(BaseModel):
    section_id: str
    url: HttpUrl
    weight: int
    selectors: List[str]
    max_items: int = 20

    @validator('weight')
    def weight_must_be_positive(cls, v):
        if v <= 0:
            raise ValueError('weight must be positive')
        return v
```

### 10. ç¼ºå°‘è‰ç¨¿å»é‡

**å•é¡Œ:** ç›¸åŒäº‹ä»¶å¯èƒ½ç”Ÿæˆå¤šæ¬¡è‰ç¨¿

**è§£æ±º:**
```python
def should_generate_draft(event_key):
    """æª¢æŸ¥æ˜¯å¦å·²æœ‰è¿‘æœŸè‰ç¨¿"""
    recent = db.get_drafts(
        event_key=event_key,
        since=now() - timedelta(hours=6)
    )
    return len(recent) == 0
```

---

## é©—è­‰ç¨‹å¼èƒ½å¦é‹ä½œçš„å¿…è¦æ­¥é©Ÿ

æˆ‘å·²ç¶“å»ºç«‹äº†å®Œæ•´çš„é©—è­‰å·¥å…·:

### 1. å¿«é€Ÿé©—è­‰ (5 åˆ†é˜)
```bash
cd /Users/nightpluie/Desktop/newsfollow
./verify.sh
```

é€™å€‹è…³æœ¬æœƒ:
- âœ“ æª¢æŸ¥ Python ç’°å¢ƒ
- âœ“ å»ºç«‹è™›æ“¬ç’°å¢ƒ
- âœ“ å®‰è£ä¾è³´å¥—ä»¶
- âœ“ é©—è­‰è¨­å®šæª”æ ¼å¼
- âœ“ åŸ·è¡Œçˆ¬èŸ²å¥åº·æª¢æŸ¥
- âœ“ æ¸¬è©¦ LLM æ•´åˆ (å¦‚æœæœ‰ API key)

### 2. è©³ç´°é©—è­‰ (30 åˆ†é˜)
åƒè€ƒ `VALIDATION_CHECKLIST.md`,æ¶µè“‹:
- Phase 1: åŸºç¤ç’°å¢ƒ
- Phase 2: çˆ¬èŸ²åŠŸèƒ½
- Phase 3: æ ¸å¿ƒåŠŸèƒ½
- Phase 4: ç™¼å¸ƒåŠŸèƒ½
- Phase 5: æŒçºŒé‹ä½œ
- Phase 6: æ“´å±•æ€§
- Phase 7: ç”Ÿç”¢å°±ç·’

### 3. é—œéµé©—è­‰é»

**æœ€é‡è¦çš„ 3 å€‹æ¸¬è©¦:**

```bash
# 1. çˆ¬èŸ²æ˜¯å¦èƒ½å–å¾—è³‡æ–™
python3 tests/test_crawler_health.py

# 2. èƒ½å¦åµæ¸¬äº‹ä»¶
python3 main.py run-once
sqlite3 newsfollow.db "SELECT COUNT(*) FROM signals;"
# æ‡‰è©² > 50

# 3. èƒ½å¦ç”Ÿæˆè‰ç¨¿
export OPENAI_API_KEY='your_key'
python3 tests/test_llm_integration.py
```

**å¦‚æœé€™ 3 å€‹éƒ½é€šé â†’ ç¨‹å¼åŸºæœ¬å¯é‹ä½œ**

---

## æ“´å±•åˆ°æ›´å¤šåª’é«”çš„å»ºè­°

### æ–°å¢åª’é«”ä¾†æºçš„æ­¥é©Ÿ:

1. **å…ˆåšé¸æ“‡å™¨æ¢å‹˜:**
```python
# explore_source.py
url = "https://new-media.com/news"
soup = BeautifulSoup(requests.get(url).text, 'html.parser')

# å˜—è©¦ä¸åŒé¸æ“‡å™¨
candidates = [
    "a.article-title",
    "h2 a",
    "main a[href*='/news/']",
    ".news-list a"
]

for sel in candidates:
    items = soup.select(sel)
    print(f"{sel}: {len(items)} items")
    for item in items[:3]:
        print(f"  - {item.get_text(strip=True)}")
```

2. **åŠ å…¥ config.yaml:**
```yaml
sources:
  - source_id: new_media
    source_name: NewMedia
    domain_contains: new-media.com
    sections:
      - section_id: homepage
        url: https://new-media.com/news
        weight: 5
        max_items: 20
        selectors:
          - "a.article-title"  # å¾æ¢å‹˜çµæœé¸æœ€ä½³çš„
```

3. **åŸ·è¡Œå¥åº·æª¢æŸ¥:**
```bash
python3 tests/test_crawler_health.py
```

4. **å¯¦éš›æ¸¬è©¦:**
```bash
python3 main.py run-once
python3 main.py list-events --limit 5
```

### å„ªå…ˆåŠ å…¥çš„åª’é«”å»ºè­°:

**Tier 1 (é‡è¦):**
- ä¸­æ™‚é›»å­å ±
- è‡ªç”±æ™‚å ±
- ä¸‰ç«‹æ–°è

**Tier 2 (æ¬¡è¦):**
- é¢¨å‚³åª’
- è˜‹æœæ—¥å ±
- NOWnews

**é¸æ“‡æ¨™æº–:**
- æ–°èæ›´æ–°é »ç‡é«˜
- é‡å¤§æ–°èå ±å°å¿«
- ç¶²ç«™çµæ§‹ç©©å®š
- æœ‰æ˜ç¢ºçš„ã€Œå³æ™‚ã€æˆ–ã€Œç†±é–€ã€å€å¡Š

---

## ç«‹å³è¡Œå‹•å»ºè­°

### ğŸš€ Phase 1: é©—è­‰ç•¶å‰ç³»çµ± (ä»Šå¤©)
```bash
./verify.sh
python3 main.py run-once
```

### ğŸ”§ Phase 2: ä¿®å¾©é—œéµç¼ºé™· (æœ¬é€±)
1. åŠ å…¥é¸æ“‡å™¨å¥åº·æª¢æŸ¥
2. åŠ å…¥éŒ¯èª¤å‘Šè­¦æ©Ÿåˆ¶
3. æ”¹ç”¨éåŒæ­¥çˆ¬èŸ² (å¦‚æœè¦æ“´å±•)

### âœ… Phase 3: åŠ å…¥æ¸¬è©¦ (ä¸‹é€±)
1. çˆ¬èŸ²å–®å…ƒæ¸¬è©¦
2. èšé¡æ¼”ç®—æ³•æ¸¬è©¦
3. ç«¯åˆ°ç«¯æ¸¬è©¦

### ğŸ“¦ Phase 4: é‡æ§‹ (2 é€±å¾Œ)
1. æ‹†åˆ† main.py æˆå¤šå€‹æ¨¡çµ„
2. åŠ å…¥ pydantic é©—è­‰
3. æ”¹é€²éŒ¯èª¤è™•ç†

### ğŸ¯ Phase 5: æ“´å±• (1 å€‹æœˆå¾Œ)
1. æ–°å¢ 3-5 å®¶åª’é«”
2. å‡ç´šåˆ° PostgreSQL (å¦‚éœ€è¦)
3. åŠ å…¥ dashboard

---

## ç¸½çµ

**ç•¶å‰ç‹€æ…‹:**
- âœ… æ ¸å¿ƒåŠŸèƒ½å®Œæ•´
- âœ… æ¶æ§‹è¨­è¨ˆåˆç†
- âš ï¸ ç¼ºå°‘æ¸¬è©¦å’Œç›£æ§
- âš ï¸ æ“´å±•æ€§æœ‰é™

**å¯ä»¥ä¸Šç·šå—?**
- **å¯¦é©—éšæ®µ:** âœ… å¯ä»¥
- **ç”Ÿç”¢ç’°å¢ƒ:** âŒ éœ€å…ˆä¿®å¾©é—œéµç¼ºé™·

**æœ€å¿«ä¸Šç·šè·¯å¾‘:**
1. åŸ·è¡Œ `./verify.sh` ç¢ºèªèƒ½é‹ä½œ
2. åŠ å…¥é¸æ“‡å™¨å¥åº·æª¢æŸ¥
3. è¨­å®šå‘Šè­¦æ©Ÿåˆ¶ (email/Slack)
4. å°è¦æ¨¡æ¸¬è©¦é‹ä½œ 1 é€±
5. è§€å¯ŸæŒ‡æ¨™å¾Œæ±ºå®šæ˜¯å¦æ“´å±•
