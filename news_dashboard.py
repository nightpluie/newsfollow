#!/usr/bin/env python3
"""
æ–°èç›£æ§å„€è¡¨æ¿ - æ¯”å° ETtoday ä¸¦ç”¨ Claude æ”¹å¯«
ä½¿ç”¨ Claude API (å« Skill æ”¯æ´çš„ fallback ç‰ˆæœ¬)
"""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from cache_manager import NewsCache

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
try:
    from dotenv import load_dotenv
    load_dotenv(override=True)  # å¾ .env æª”æ¡ˆè¼‰å…¥ç’°å¢ƒè®Šæ•¸ï¼ˆè¦†è“‹ shell ç’°å¢ƒè®Šæ•¸ï¼‰
except ImportError:
    print("âš ï¸  æœªå®‰è£ python-dotenvï¼Œè«‹åŸ·è¡Œ: pip install python-dotenv")

import anthropic
import requests
import yaml
from bs4 import BeautifulSoup
from flask import Flask, jsonify, render_template, request
from flask_cors import CORS

# è¼‰å…¥åŸæœ‰çš„å‡½æ•¸
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from main import (
    RequestsCrawler,
    Signal,
    extract_signals,
    normalize_title,
    now_iso,
)
from hybrid_similarity import HybridSimilarityChecker

app = Flask(__name__)
CORS(app)

# API Keys å¾ç’°å¢ƒè®Šæ•¸è®€å–
CLAUDE_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")  # é è¨­ä½¿ç”¨ gpt-4o-mini

# è¼‰å…¥å¯«ä½œæŠ€èƒ½ (å¯é¸)
SKILL_PATH = os.getenv("SKILL_PATH", "./SKILL.md")
TCY_SKILL = ""
if os.path.exists(SKILL_PATH):
    try:
        with open(SKILL_PATH, 'r', encoding='utf-8') as f:
            TCY_SKILL = f.read()
        print(f"âœ… å·²è¼‰å…¥å¯«ä½œæŠ€èƒ½: {SKILL_PATH}")
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•è¼‰å…¥æŠ€èƒ½æª”æ¡ˆ: {e}")


@dataclass
class NewsItem:
    """æ–°èé …ç›®"""
    source: str
    title: str
    url: str
    normalized_title: str
    crawled_at: str
    section: str = 'homepage'
    weight: int = 5


class NewsDashboard:
    def __init__(self, config_path: str = "./config.yaml"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.crawler = RequestsCrawler()
        if not CLAUDE_API_KEY:
            print("âš ï¸  æœªè¨­å®š ANTHROPIC_API_KEY ç’°å¢ƒè®Šæ•¸ï¼Œæ”¹å¯«åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
        self.claude = anthropic.Anthropic(api_key=CLAUDE_API_KEY) if CLAUDE_API_KEY else None

        # åˆå§‹åŒ–æ··åˆç›¸ä¼¼åº¦æª¢æŸ¥å™¨ï¼ˆæ¼”ç®—æ³• + LLMï¼‰
        self.similarity_checker = HybridSimilarityChecker(
            api_key=OPENAI_API_KEY,
            model=OPENAI_MODEL,
            enable_llm=True
        )

        # åˆå§‹åŒ–å¿«å–ç®¡ç†å™¨ï¼ˆETtoday å¿«å– 5 åˆ†é˜ï¼‰
        self.cache = NewsCache(cache_dir="./cache", ttl_minutes=5)
        print("âœ… å¿«å–ç³»çµ±å·²å•Ÿç”¨ï¼ˆTTL: 5 åˆ†é˜ï¼‰")

    def crawl_source(self, source_config: Dict) -> List[NewsItem]:
        """çˆ¬å–å–®ä¸€åª’é«”ä¾†æº"""
        items = []

        for section in source_config['sections']:
            try:
                html = self.crawler.fetch_html(section['url'])
                signals = extract_signals(
                    html=html,
                    base_url=section['url'],
                    source_id=source_config['source_id'],
                    source_name=source_config['source_name'],
                    section_id=section['section_id'],
                    domain_contains=source_config.get('domain_contains', ''),
                    selectors=section.get('selectors', []),
                    weight=section.get('weight', 1),
                    crawled_at=now_iso(),
                    max_items=section.get('max_items', 20),
                )

                for sig in signals:
                    items.append(NewsItem(
                        source=sig.source_name,
                        title=sig.title,
                        url=sig.url,
                        normalized_title=sig.normalized_title,
                        crawled_at=sig.crawled_at,
                        section=sig.section_id,
                        weight=sig.weight,
                    ))

            except Exception as e:
                print(f"Error crawling {source_config['source_id']}/{section['section_id']}: {e}")

        return items

    def crawl_ettoday(self) -> List[NewsItem]:
        """çˆ¬å– ETtoday æ–°èï¼ˆå¸¶å¿«å–ï¼‰"""
        # æª¢æŸ¥å¿«å–
        cached_data = self.cache.get('ettoday')
        if cached_data:
            cache_info = self.cache.get_info('ettoday')
            print(f"âœ… ä½¿ç”¨ ETtoday å¿«å–ï¼ˆ{cache_info['age_seconds']:.0f}ç§’å‰ï¼‰")
            # å°‡å­—å…¸è½‰å› NewsItem ç‰©ä»¶
            return [NewsItem(**item) for item in cached_data]

        print("ğŸ”„ çˆ¬å– ETtoday æ–°èï¼ˆå¿«å–éæœŸæˆ–ä¸å­˜åœ¨ï¼‰...")
        items = []

        urls = [
            "https://www.ettoday.net/news/news-list.htm",
            "https://www.ettoday.net/news/focus/ç„¦é»æ–°è/",
            "https://www.ettoday.net/news/hot-news.htm",
        ]

        for url in urls:
            try:
                html = self.crawler.fetch_html(url)
                soup = BeautifulSoup(html, 'html.parser')

                selectors = [
                    "h3 a",
                    ".part_list_2 h3 a",
                    ".piece h3 a",
                ]

                for selector in selectors:
                    for link in soup.select(selector):
                        title = link.get_text(strip=True)
                        href = link.get('href', '')

                        if not title or len(title) < 8:
                            continue

                        full_url = href if href.startswith('http') else f"https://www.ettoday.net{href}"

                        items.append(NewsItem(
                            source="ETtoday",
                            title=title,
                            url=full_url,
                            normalized_title=normalize_title(title),
                            crawled_at=now_iso(),
                        ))

            except Exception as e:
                print(f"Error crawling ETtoday {url}: {e}")

        # å»é‡
        seen = set()
        unique_items = []
        for item in items:
            key = item.normalized_title
            if key not in seen:
                seen.add(key)
                unique_items.append(item)

        # å„²å­˜åˆ°å¿«å–ï¼ˆè½‰ç‚ºå­—å…¸æ ¼å¼ï¼‰
        cache_data = [
            {
                'source': item.source,
                'title': item.title,
                'url': item.url,
                'normalized_title': item.normalized_title,
                'crawled_at': item.crawled_at,
                'section': item.section,
                'weight': item.weight,
            }
            for item in unique_items
        ]
        self.cache.set('ettoday', cache_data)
        print(f"ğŸ’¾ å·²å¿«å– ETtoday æ–°èï¼ˆ{len(cache_data)} å‰‡ï¼‰")

        return unique_items

    def find_missing_news(self, all_source_items: Dict[str, List[NewsItem]],
                         ettoday_items: List[NewsItem]) -> List[Dict]:
        """
        æ‰¾å‡º ETtoday æ²’æœ‰çš„æ–°è
        ä½¿ç”¨æ··åˆç­–ç•¥ï¼ˆæ¼”ç®—æ³• + LLMï¼‰é€²è¡Œç›¸ä¼¼åº¦æ¯”å°
        ä¸¦å°‡ç›¸åŒæ–°èåˆ†ç¾¤é¡¯ç¤º
        """
        from main import title_similarity
        from news_importance import calculate_news_importance, format_star_rating

        # æ”¶é›† ETtoday æ‰€æœ‰æ¨™é¡Œï¼ˆç”¨æ–¼æ··åˆæ¯”å°ï¼‰
        ettoday_titles_list = [item.title for item in ettoday_items]

        # æ”¶é›†æ‰€æœ‰ä¸åœ¨ ETtoday çš„æ–°èï¼ˆä½¿ç”¨æ··åˆç›¸ä¼¼åº¦æ¯”å°ï¼‰
        self.similarity_checker.reset_statistics()
        missing_items = []

        for source_name, items in all_source_items.items():
            for item in items:
                # ä½¿ç”¨æ··åˆç­–ç•¥æª¢æŸ¥æ˜¯å¦åœ¨ ETtoday ä¸­å­˜åœ¨
                is_in_ettoday = self.similarity_checker.batch_check(
                    candidate_title=item.title,
                    reference_titles=ettoday_titles_list
                )

                # åªæœ‰ç•¶ç¢ºå®šä¸åœ¨ ETtoday æ™‚ï¼Œæ‰åŠ å…¥ç¼ºå°‘åˆ—è¡¨
                if not is_in_ettoday:
                    missing_items.append(item)

        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = self.similarity_checker.get_statistics()
        print(f"ğŸ“Š ç›¸ä¼¼åº¦æ¯”å°çµ±è¨ˆ: LLM èª¿ç”¨æ¬¡æ•¸ = {stats['llm_call_count']}")

        # ä½¿ç”¨æ”¹é€²çš„ç›¸ä¼¼åº¦æ¼”ç®—æ³•é€²è¡Œç¾¤é›†ï¼ˆå‚³éæ€§ç¾¤é›†ï¼‰
        clusters = []
        for item in missing_items:
            title = item.title
            placed = False

            # æª¢æŸ¥æ˜¯å¦èˆ‡ç¾æœ‰ç¾¤é›†ä¸­çš„ä»»ä½•æ–°èç›¸ä¼¼
            for i, cluster in enumerate(clusters):
                # èˆ‡ç¾¤é›†ä¸­çš„æ¯å€‹é …ç›®æ¯”è¼ƒ
                for existing_item in cluster:
                    # ä½¿ç”¨ 0.47 é–¾å€¼ï¼ˆæ¯” 0.5 ç¨ä½ï¼Œå› ç‚ºé€™æ˜¯æœ€çµ‚é¡¯ç¤ºç”¨ï¼‰
                    if title_similarity(title, existing_item.title) >= 0.47:
                        clusters[i].append(item)
                        placed = True
                        break
                if placed:
                    break

            if not placed:
                clusters.append([item])

        # ç‚ºæ¯å€‹ç¾¤é›†å»ºç«‹æ–°èè³‡è¨Š
        news_by_cluster = []
        for cluster in clusters:
            # é¸æ“‡æœ€é•·çš„æ¨™é¡Œä½œç‚ºä»£è¡¨æ¨™é¡Œ
            canonical_title = max((item.title for item in cluster), key=len)
            canonical_url = cluster[0].url

            # æ”¶é›†æ‰€æœ‰ä¾†æºçš„è©³ç´°è³‡è¨Šï¼ˆä½¿ç”¨å­—å…¸å»é‡ï¼‰
            sources = []
            source_details_dict = {}
            sections_info = []

            for item in cluster:
                sources.append(item.source)

                # å¦‚æœè©²ä¾†æºé‚„æ²’è¨˜éŒ„ï¼Œæˆ–æ–°æ¨™é¡Œæ›´é•·ï¼Œå‰‡æ›´æ–°
                if item.source not in source_details_dict or len(item.title) > len(source_details_dict[item.source]['title']):
                    source_details_dict[item.source] = {
                        'source': item.source,
                        'title': item.title,
                        'url': item.url,
                    }

                sections_info.append({
                    'source': item.source,
                    'section': getattr(item, 'section', 'homepage'),
                    'weight': getattr(item, 'weight', 5),
                })

            # å°‡å­—å…¸è½‰ç‚ºåˆ—è¡¨ï¼ˆæ¯å€‹ä¾†æºåªä¿ç•™ä¸€å‰‡ï¼‰
            source_details = list(source_details_dict.values())

            # è¨ˆç®—é‡è¦æ€§è©•åˆ†
            importance = calculate_news_importance(canonical_title, sources, sections_info)
            star_rating = format_star_rating(importance['star_rating'])

            news_by_cluster.append({
                'title': canonical_title,
                'url': canonical_url,
                'normalized_title': cluster[0].normalized_title,
                'crawled_at': cluster[0].crawled_at,
                'sources': sources,  # ç°¡å–®çš„ä¾†æºåç¨±åˆ—è¡¨ï¼ˆç”¨æ–¼è©•åˆ†ï¼‰
                'source_details': source_details,  # è©³ç´°çš„ä¾†æºè³‡è¨Šï¼ˆç”¨æ–¼å‰ç«¯é¡¯ç¤ºï¼Œå·²å»é‡ï¼‰
                'sections_info': sections_info,
                'importance': importance,
                'star_rating': star_rating,
                'total_score': importance['total_score'],
            })

        # æŒ‰é‡è¦æ€§è©•åˆ†æ’åºï¼ˆé«˜åˆ†åœ¨å‰ï¼‰
        news_by_cluster.sort(key=lambda x: x['total_score'], reverse=True)

        return news_by_cluster

    def clean_markdown(self, text: str) -> str:
        """ç§»é™¤ Markdown æ ¼å¼æ¨™è¨˜"""
        import re
        # ç§»é™¤ç²—é«” **text** æˆ– __text__
        text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
        text = re.sub(r'__(.+?)__', r'\1', text)
        # ç§»é™¤æ–œé«” *text* æˆ– _text_
        text = re.sub(r'\*(.+?)\*', r'\1', text)
        text = re.sub(r'_(.+?)_', r'\1', text)
        # ç§»é™¤æ¨™é¡Œæ¨™è¨˜ #
        text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
        return text

    def rewrite_with_claude(self, original_title: str, original_url: str) -> Dict:
        """ä½¿ç”¨ Claude API æ”¹å¯«æ–°è (ä½¿ç”¨å”é®å®‡æŠ€èƒ½æŒ‡å¼•)"""
        try:
            # å…ˆå˜—è©¦æŠ“å–åŸæ–‡å…§å®¹
            try:
                html = self.crawler.fetch_html(original_url)
                soup = BeautifulSoup(html, 'html.parser')
                paragraphs = soup.find_all('p')
                original_content = '\n'.join([p.get_text(strip=True) for p in paragraphs[:10]])
            except:
                original_content = ""

            # æº–å‚™åŒ…å«å”é®å®‡æŠ€èƒ½çš„ system prompt
            system_prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–°èè¨˜è€…ã€‚è«‹åš´æ ¼éµå¾ªä»¥ä¸‹å”é®å®‡çš„æ–°èå ±å°å¯«ä½œæŠ€èƒ½:

{TCY_SKILL}

é‡è¦æé†’:
1. åš´æ ¼ä¾æ“šç´ ææ’°å¯« - åªèƒ½æ ¹æ“šæä¾›çš„ç´ æ,ç¦æ­¢è‡ªè¡Œæ£æƒ³æˆ–ç·¨é€ 
2. å€’é‡‘å­—å¡”çµæ§‹ - æœ€é‡è¦è³‡è¨Šåœ¨å‰,ä¾é‡è¦æ€§éæ¸›
3. é‡‘å­—å¡”åŸç† - æ¯æ®µé¦–å¥æ˜¯æ ¸å¿ƒè«–é»,å¾ŒçºŒå…§å®¹æ”¯æ’è©²è«–é»
4. å°è¨€æ¶µè“‹ 5W1H - ä½•æ™‚ã€ä½•åœ°ã€ä½•äººã€ä½•äº‹ã€ç‚ºä½•ã€å¦‚ä½•
5. æ•¸æ“šå…ˆè¡Œ - ç”¨å…·é«”æ•¸å­—ã€çµ±è¨ˆè³‡æ–™é–‹å ´
6. 250å­—å°è¨€ - ç²¾ç…‰æ ¸å¿ƒé‡é»,ä¸è¶…é250å­—

ã€æ ¼å¼è¦æ±‚ - æ¥µåº¦é‡è¦ã€‘
- çµ•å°ä¸å¯ä½¿ç”¨ Markdown æ ¼å¼
- ä¸å¯ä½¿ç”¨ **ç²—é«”**ã€*æ–œé«”*ã€# æ¨™é¡Œç­‰ä»»ä½• Markdown èªæ³•
- ä½¿ç”¨ç´”æ–‡å­—è¼¸å‡º,ä¸éœ€ä»»ä½•æ ¼å¼æ¨™è¨˜
- å¦‚éœ€å¼·èª¿,ä½¿ç”¨ã€Œã€æˆ–ç›´æ¥åŠ å¼·èªæ°£çš„æ–‡å­—å³å¯
"""

            user_prompt = f"""è«‹å°‡ä»¥ä¸‹æ–°èæ”¹å¯«æˆ ETtoday é¢¨æ ¼çš„å°ˆæ¥­å ±å°:

**åŸå§‹æ¨™é¡Œ:** {original_title}
**åŸå§‹ä¾†æº:** {original_url}

**åŸå§‹å…§å®¹:**
{original_content if original_content else "ï¼ˆç„¡æ³•å–å¾—å®Œæ•´å…§å®¹,è«‹æ ¹æ“šæ¨™é¡Œæ¨æ¸¬ä¸¦æ”¹å¯«,ä½†è«‹æ˜ç¢ºæ¨™ç¤ºç‚ºæ¨æ¸¬å…§å®¹ï¼‰"}

è«‹æä¾›:
1. æ–°èæ¨™é¡Œ (ç°¡æ½”æœ‰åŠ›,50å­—ä»¥å…§)
2. å°è¨€ (250å­—ä»¥å…§,æ¶µè“‹ 5W1H,æ•¸æ“šå…ˆè¡Œ)
3. å®Œæ•´å…§æ–‡ (ä¾å€’é‡‘å­—å¡”çµæ§‹,ç´„ 400-600 å­—)

è«‹ä»¥ JSON æ ¼å¼å›å‚³:
{{
    "title": "æ”¹å¯«å¾Œçš„æ¨™é¡Œ",
    "lead": "å°è¨€å…§å®¹ (250å­—å…§)",
    "body": "å®Œæ•´å…§æ–‡ (400-600å­—)"
}}
"""

            # å‘¼å« Claude API
            message = self.claude.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=2000,
                temperature=0.7,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )

            # è§£æå›æ‡‰
            response_text = message.content[0].text

            # å˜—è©¦æå– JSON
            try:
                if "```json" in response_text:
                    response_text = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    response_text = response_text.split("```")[1].split("```")[0]

                result = json.loads(response_text.strip())

                # æ¸…ç† Markdown æ ¼å¼
                clean_title = self.clean_markdown(result.get('title', ''))
                clean_lead = self.clean_markdown(result.get('lead', ''))
                clean_body = self.clean_markdown(result.get('body', ''))

                return {
                    'success': True,
                    'title': clean_title,
                    'lead': clean_lead,
                    'body': clean_body,
                    'original_title': original_title,
                    'original_url': original_url,
                    'model': 'claude-sonnet-4-20250514',
                    'method': 'system_prompt_with_skill',
                }

            except json.JSONDecodeError:
                # æ¸…ç† Markdown æ ¼å¼
                clean_text = self.clean_markdown(response_text)

                return {
                    'success': True,
                    'title': original_title,
                    'lead': clean_text[:250],
                    'body': clean_text,
                    'original_title': original_title,
                    'original_url': original_url,
                    'model': 'claude-sonnet-4-20250514',
                    'method': 'system_prompt_with_skill',
                    'note': 'Failed to parse JSON, returning raw text',
                }

        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'original_title': original_title,
                'original_url': original_url,
            }


# å»ºç«‹å…¨åŸŸå¯¦ä¾‹
dashboard = NewsDashboard()


@app.route('/')
def index():
    """é¦–é """
    return render_template('dashboard.html')


@app.route('/api/crawl', methods=['POST'])
def api_crawl():
    """çˆ¬å–æ‰€æœ‰ä¾†æºï¼ˆå¹³è¡ŒåŸ·è¡Œï¼‰"""
    try:
        # å®šç¾©çˆ¬å–ä»»å‹™
        def crawl_udn():
            config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'udn'), None)
            return ('UDN', dashboard.crawl_source(config) if config else [])

        def crawl_tvbs():
            config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'tvbs'), None)
            return ('TVBS', dashboard.crawl_source(config) if config else [])

        def crawl_chinatimes():
            config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'chinatimes'), None)
            return ('ä¸­æ™‚æ–°èç¶²', dashboard.crawl_source(config) if config else [])

        def crawl_setn():
            config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'setn'), None)
            return ('ä¸‰ç«‹æ–°èç¶²', dashboard.crawl_source(config) if config else [])

        def crawl_et():
            return ('ETtoday', dashboard.crawl_ettoday())

        # å¹³è¡Œçˆ¬å–æ‰€æœ‰ä¾†æºï¼ˆæœ€å¤š 5 å€‹åŒæ™‚åŸ·è¡Œï¼‰
        results = {}
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            futures = [
                executor.submit(crawl_udn),
                executor.submit(crawl_tvbs),
                executor.submit(crawl_chinatimes),
                executor.submit(crawl_setn),
                executor.submit(crawl_et),
            ]

            # æ”¶é›†çµæœ
            for future in as_completed(futures):
                source_name, items = future.result()
                results[source_name] = items

        # æå–çµæœ
        udn_items = results.get('UDN', [])
        tvbs_items = results.get('TVBS', [])
        chinatimes_items = results.get('ä¸­æ™‚æ–°èç¶²', [])
        setn_items = results.get('ä¸‰ç«‹æ–°èç¶²', [])
        ettoday_items = results.get('ETtoday', [])

        # çµ„åˆæ‰€æœ‰ä¾†æºçš„å­—å…¸
        all_source_items = {
            'UDN': udn_items,
            'TVBS': tvbs_items,
            'ä¸­æ™‚æ–°èç¶²': chinatimes_items,
            'ä¸‰ç«‹æ–°èç¶²': setn_items,
        }

        # æ‰¾å‡º ETtoday ç¼ºå°‘çš„æ–°èï¼ˆä½¿ç”¨æ··åˆç›¸ä¼¼åº¦ç­–ç•¥ï¼‰
        missing_news = dashboard.find_missing_news(all_source_items, ettoday_items)

        # å–å¾— LLM èª¿ç”¨æ¬¡æ•¸çµ±è¨ˆ
        llm_calls = dashboard.similarity_checker.llm_call_count

        return jsonify({
            'success': True,
            'udn': [{'source': i.source, 'title': i.title, 'url': i.url} for i in udn_items],
            'tvbs': [{'source': i.source, 'title': i.title, 'url': i.url} for i in tvbs_items],
            'ä¸­æ™‚æ–°èç¶²': [{'source': i.source, 'title': i.title, 'url': i.url} for i in chinatimes_items],
            'ä¸‰ç«‹æ–°èç¶²': [{'source': i.source, 'title': i.title, 'url': i.url} for i in setn_items],
            'ettoday': [{'source': i.source, 'title': i.title, 'url': i.url} for i in ettoday_items],
            'missing': missing_news,
            'llm_calls': llm_calls,
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/rewrite', methods=['POST'])
def api_rewrite():
    """æ”¹å¯«å–®å‰‡æ–°è"""
    try:
        data = request.json
        title = data.get('title', '')
        url = data.get('url', '')

        if not title:
            return jsonify({'success': False, 'error': 'Title is required'}), 400

        result = dashboard.rewrite_with_claude(title, url)
        return jsonify(result)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


if __name__ == '__main__':
    os.makedirs('templates', exist_ok=True)

    print("=" * 60)
    print("ğŸš€ æ–°èç›£æ§å„€è¡¨æ¿ v1.1 (Prototype) å•Ÿå‹•ä¸­...")
    print("ğŸ“ è¨ªå•: http://localhost:8080")
    print("=" * 60)
    print("ğŸ’¡ ä½¿ç”¨æ–¹å¼: System Prompt + å”é®å®‡å¯«ä½œæŠ€èƒ½")
    print("   (Skills API åœ¨ Python SDK ä¸­å°šæœªå®Œå…¨æ”¯æ´)")
    print("=" * 60)

    app.run(debug=True, host='0.0.0.0', port=8080)
