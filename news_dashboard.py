#!/usr/bin/env python3
"""
æ–°èç›£æ§å„€è¡¨æ¿ - æ¯”å° ETtoday ä¸¦ç”¨ Claude æ”¹å¯«
ä½¿ç”¨ Claude API (å« Skill æ”¯æ´çš„ fallback ç‰ˆæœ¬)
"""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from cache_manager import NewsCache

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
try:
    from dotenv import load_dotenv
    load_dotenv(override=True)  # å¾ .env æª”æ¡ˆè¼‰å…¥ç’°å¢ƒè®Šæ•¸ï¼ˆè¦†è“‹ shell ç’°å¢ƒè®Šæ•¸ï¼‰
except ImportError:
    print("âš ï¸  æœªå®‰è£ python-dotenvï¼Œè«‹åŸ·è¡Œ: pip install python-dotenv")

import anthropic
import requests
import yaml
from bs4 import BeautifulSoup
from flask import Flask, jsonify, render_template, request
from flask_cors import CORS

# è¼‰å…¥åŸæœ‰çš„å‡½æ•¸
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from main import (
    RequestsCrawler,
    Signal,
    extract_signals,
    normalize_title,
    now_iso,
)
from hybrid_similarity import HybridSimilarityChecker

app = Flask(__name__)
CORS(app)

# API Keys å¾ç’°å¢ƒè®Šæ•¸è®€å–
CLAUDE_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")  # é è¨­ä½¿ç”¨ gpt-4o-mini

# è¼‰å…¥å¯«ä½œæŠ€èƒ½ (å¯é¸)
SKILL_PATH = os.getenv("SKILL_PATH", "./SKILL.md")
TCY_SKILL = ""
if os.path.exists(SKILL_PATH):
    try:
        with open(SKILL_PATH, 'r', encoding='utf-8') as f:
            TCY_SKILL = f.read()
        print(f"âœ… å·²è¼‰å…¥å¯«ä½œæŠ€èƒ½: {SKILL_PATH}")
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•è¼‰å…¥æŠ€èƒ½æª”æ¡ˆ: {e}")


@dataclass
class NewsItem:
    """æ–°èé …ç›®"""
    source: str
    title: str
    url: str
    normalized_title: str
    crawled_at: str
    section: str = 'homepage'
    weight: int = 5


class NewsDashboard:
    def __init__(self, config_path: str = "./config.yaml"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.crawler = RequestsCrawler()
        if not CLAUDE_API_KEY:
            print("âš ï¸  æœªè¨­å®š ANTHROPIC_API_KEY ç’°å¢ƒè®Šæ•¸ï¼Œæ”¹å¯«åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
        self.claude = anthropic.Anthropic(api_key=CLAUDE_API_KEY) if CLAUDE_API_KEY else None

        # åˆå§‹åŒ–æ··åˆç›¸ä¼¼åº¦æª¢æŸ¥å™¨ï¼ˆæ¼”ç®—æ³• + LLMï¼‰
        self.similarity_checker = HybridSimilarityChecker(
            api_key=OPENAI_API_KEY,
            model=OPENAI_MODEL,  # ä½¿ç”¨ç’°å¢ƒè®Šæ•¸é…ç½®çš„æ¨¡å‹ï¼ˆé è¨­ gpt-4.1-nanoï¼‰
            enable_llm=True,
            timeout=10  # API è«‹æ±‚è¶…æ™‚ 10 ç§’ï¼ˆé˜²æ­¢å–®æ¬¡è«‹æ±‚å¡ä½ï¼‰
        )

        # åˆå§‹åŒ–å¿«å–ç®¡ç†å™¨ï¼ˆETtoday å¿«å– 5 åˆ†é˜ï¼‰
        self.cache = NewsCache(cache_dir="./cache", ttl_minutes=5)
        print("âœ… å¿«å–ç³»çµ±å·²å•Ÿç”¨ï¼ˆTTL: 5 åˆ†é˜ï¼‰")

    def crawl_source(self, source_config: Dict) -> List[NewsItem]:
        """çˆ¬å–å–®ä¸€åª’é«”ä¾†æº"""
        items = []

        for section in source_config['sections']:
            try:
                html = self.crawler.fetch_html(section['url'])
                signals = extract_signals(
                    html=html,
                    base_url=section['url'],
                    source_id=source_config['source_id'],
                    source_name=source_config['source_name'],
                    section_id=section['section_id'],
                    domain_contains=source_config.get('domain_contains', ''),
                    selectors=section.get('selectors', []),
                    weight=section.get('weight', 1),
                    crawled_at=now_iso(),
                    max_items=section.get('max_items', 20),
                    exclude_patterns=source_config.get('exclude_patterns', []),
                )

                for sig in signals:
                    items.append(NewsItem(
                        source=sig.source_name,
                        title=sig.title,
                        url=sig.url,
                        normalized_title=sig.normalized_title,
                        crawled_at=sig.crawled_at,
                        section=sig.section_id,
                        weight=sig.weight,
                    ))

            except Exception as e:
                print(f"Error crawling {source_config['source_id']}/{section['section_id']}: {e}")

        return items

    def crawl_ettoday(self) -> List[NewsItem]:
        """çˆ¬å– ETtoday æ–°èï¼ˆå¸¶å¿«å–ï¼‰"""
        # æª¢æŸ¥å¿«å–
        cached_data = self.cache.get('ettoday')
        if cached_data:
            cache_info = self.cache.get_info('ettoday')
            print(f"âœ… ä½¿ç”¨ ETtoday å¿«å–ï¼ˆ{cache_info['age_seconds']:.0f}ç§’å‰ï¼‰")
            # å°‡å­—å…¸è½‰å› NewsItem ç‰©ä»¶
            return [NewsItem(**item) for item in cached_data]

        print("ğŸ”„ çˆ¬å– ETtoday æ–°èï¼ˆå¿«å–éæœŸæˆ–ä¸å­˜åœ¨ï¼‰...")
        items = []

        urls = [
            "https://www.ettoday.net/news/news-list.htm",
            "https://www.ettoday.net/news/focus/ç„¦é»æ–°è/",
            "https://www.ettoday.net/news/hot-news.htm",
        ]

        for url in urls:
            try:
                html = self.crawler.fetch_html(url)
                soup = BeautifulSoup(html, 'html.parser')

                selectors = [
                    "h3 a",
                    ".part_list_2 h3 a",
                    ".piece h3 a",
                ]

                for selector in selectors:
                    for link in soup.select(selector):
                        title = link.get_text(strip=True)
                        href = link.get('href', '')

                        if not title or len(title) < 8:
                            continue

                        full_url = href if href.startswith('http') else f"https://www.ettoday.net{href}"

                        items.append(NewsItem(
                            source="ETtoday",
                            title=title,
                            url=full_url,
                            normalized_title=normalize_title(title),
                            crawled_at=now_iso(),
                        ))

            except Exception as e:
                print(f"Error crawling ETtoday {url}: {e}")

        # å»é‡
        seen = set()
        unique_items = []
        for item in items:
            key = item.normalized_title
            if key not in seen:
                seen.add(key)
                unique_items.append(item)

        # å„²å­˜åˆ°å¿«å–ï¼ˆè½‰ç‚ºå­—å…¸æ ¼å¼ï¼‰
        cache_data = [
            {
                'source': item.source,
                'title': item.title,
                'url': item.url,
                'normalized_title': item.normalized_title,
                'crawled_at': item.crawled_at,
                'section': item.section,
                'weight': item.weight,
            }
            for item in unique_items
        ]
        self.cache.set('ettoday', cache_data)
        print(f"ğŸ’¾ å·²å¿«å– ETtoday æ–°èï¼ˆ{len(cache_data)} å‰‡ï¼‰")

        return unique_items

    def find_missing_news(self, all_source_items: Dict[str, List[NewsItem]],
                         ettoday_items: List[NewsItem]) -> List[Dict]:
        """
        æ‰¾å‡º ETtoday æ²’æœ‰çš„æ–°è
        ä½¿ç”¨æ··åˆç­–ç•¥ï¼ˆæ¼”ç®—æ³• + LLMï¼‰é€²è¡Œç›¸ä¼¼åº¦æ¯”å°
        ä¸¦å°‡ç›¸åŒæ–°èåˆ†ç¾¤é¡¯ç¤º
        """
        from main import title_similarity, compute_title_features
        from news_importance import calculate_news_importance, format_star_rating

        # é è¨ˆç®— ETtoday æ‰€æœ‰æ¨™é¡Œç‰¹å¾µï¼ˆç”¨æ–¼æ··åˆæ¯”å°ï¼‰
        # é€™èƒ½å¤§å¹…æ¸›å°‘é‡è¤‡å»ºç«‹ Set/Counter çš„è¨˜æ†¶é«”é–‹éŠ·
        ettoday_features_list = [compute_title_features(item.title) for item in ettoday_items]

        # æ”¶é›†æ‰€æœ‰ä¸åœ¨ ETtoday çš„æ–°èï¼ˆä½¿ç”¨æ··åˆç›¸ä¼¼åº¦æ¯”å°ï¼‰
        self.similarity_checker.reset_statistics()
        missing_items = []

        # ç”¨æ–¼ç¾¤é›†çš„é …ç›®åˆ—è¡¨ï¼ˆå„²å­˜ (item, features)ï¼‰
        missing_items_with_features = []

        for source_name, items in all_source_items.items():
            for item in items:
                # é è¨ˆç®—å€™é¸æ¨™é¡Œç‰¹å¾µ
                candidate_features = compute_title_features(item.title)
                
                # ä½¿ç”¨æ··åˆç­–ç•¥æª¢æŸ¥æ˜¯å¦åœ¨ ETtoday ä¸­å­˜åœ¨
                # å‚³éé è¨ˆç®—çš„ç‰¹å¾µç‰©ä»¶
                is_in_ettoday = self.similarity_checker.batch_check(
                    candidate_title=candidate_features,
                    reference_titles=ettoday_features_list
                )

                # åªæœ‰ç•¶ç¢ºå®šä¸åœ¨ ETtoday æ™‚ï¼Œæ‰åŠ å…¥ç¼ºå°‘åˆ—è¡¨
                if not is_in_ettoday:
                    missing_items.append(item)
                    missing_items_with_features.append((item, candidate_features))

        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = self.similarity_checker.get_statistics()
        print(f"ğŸ“Š ç›¸ä¼¼åº¦æ¯”å°çµ±è¨ˆ: LLM èª¿ç”¨æ¬¡æ•¸ = {stats['llm_call_count']}")

        # ä½¿ç”¨æ”¹é€²çš„ç›¸ä¼¼åº¦æ¼”ç®—æ³•é€²è¡Œç¾¤é›†ï¼ˆå‚³éæ€§ç¾¤é›†ï¼‰
        # clusters å„²å­˜çµæ§‹: List[List[Tuple[NewsItem, TitleFeatures]]]
        clusters = []
        for item, features in missing_items_with_features:
            placed = False

            # æª¢æŸ¥æ˜¯å¦èˆ‡ç¾æœ‰ç¾¤é›†ä¸­çš„ä»»ä½•æ–°èç›¸ä¼¼
            for i, cluster in enumerate(clusters):
                # èˆ‡ç¾¤é›†ä¸­çš„æ¯å€‹é …ç›®æ¯”è¼ƒ
                for existing_item, existing_features in cluster:
                    # ä½¿ç”¨ 0.47 é–¾å€¼ï¼ˆæ¯” 0.5 ç¨ä½ï¼Œå› ç‚ºé€™æ˜¯æœ€çµ‚é¡¯ç¤ºç”¨ï¼‰
                    # ç›´æ¥ä½¿ç”¨ç‰¹å¾µé€²è¡Œæ¯”å°
                    if title_similarity(features, existing_features) >= 0.47:
                        clusters[i].append((item, features))
                        placed = True
                        break
                if placed:
                    break

            if not placed:
                clusters.append([(item, features)])
        
        # é‚„åŸ clusters ç‚ºç´” NewsItem åˆ—è¡¨ä»¥ä¾¿å¾ŒçºŒè™•ç†
        news_clusters = [[pair[0] for pair in cluster] for cluster in clusters]

        # ç‚ºæ¯å€‹ç¾¤é›†å»ºç«‹æ–°èè³‡è¨Š
        news_by_cluster = []
        for cluster in news_clusters:
            # é¸æ“‡æœ€é•·çš„æ¨™é¡Œä½œç‚ºä»£è¡¨æ¨™é¡Œ
            canonical_title = max((item.title for item in cluster), key=len)
            canonical_url = cluster[0].url

            # æ”¶é›†æ‰€æœ‰ä¾†æºçš„è©³ç´°è³‡è¨Šï¼ˆä½¿ç”¨å­—å…¸å»é‡ï¼‰
            sources = []
            source_details_dict = {}
            sections_info = []

            for item in cluster:
                sources.append(item.source)

                # å¦‚æœè©²ä¾†æºé‚„æ²’è¨˜éŒ„ï¼Œæˆ–æ–°æ¨™é¡Œæ›´é•·ï¼Œå‰‡æ›´æ–°
                if item.source not in source_details_dict or len(item.title) > len(source_details_dict[item.source]['title']):
                    source_details_dict[item.source] = {
                        'source': item.source,
                        'title': item.title,
                        'url': item.url,
                    }

                sections_info.append({
                    'source': item.source,
                    'section': getattr(item, 'section', 'homepage'),
                    'weight': getattr(item, 'weight', 5),
                })

            # å°‡å­—å…¸è½‰ç‚ºåˆ—è¡¨ï¼ˆæ¯å€‹ä¾†æºåªä¿ç•™ä¸€å‰‡ï¼‰
            source_details = list(source_details_dict.values())

            # è¨ˆç®—é‡è¦æ€§è©•åˆ†
            importance = calculate_news_importance(canonical_title, sources, sections_info)
            star_rating = format_star_rating(importance['star_rating'])

            news_by_cluster.append({
                'title': canonical_title,
                'url': canonical_url,
                'normalized_title': cluster[0].normalized_title,
                'crawled_at': cluster[0].crawled_at,
                'sources': sources,  # ç°¡å–®çš„ä¾†æºåç¨±åˆ—è¡¨ï¼ˆç”¨æ–¼è©•åˆ†ï¼‰
                'source_details': source_details,  # è©³ç´°çš„ä¾†æºè³‡è¨Šï¼ˆç”¨æ–¼å‰ç«¯é¡¯ç¤ºï¼Œå·²å»é‡ï¼‰
                'sections_info': sections_info,
                'importance': importance,
                'star_rating': star_rating,
                'total_score': importance['total_score'],
            })

        # æŒ‰é‡è¦æ€§è©•åˆ†æ’åºï¼ˆé«˜åˆ†åœ¨å‰ï¼‰
        news_by_cluster.sort(key=lambda x: x['total_score'], reverse=True)

        return news_by_cluster

    def clean_markdown(self, text: str) -> str:
        """ç§»é™¤ Markdown æ ¼å¼æ¨™è¨˜"""
        import re
        # ç§»é™¤ç²—é«” **text** æˆ– __text__
        text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
        text = re.sub(r'__(.+?)__', r'\1', text)
        # ç§»é™¤æ–œé«” *text* æˆ– _text_
        text = re.sub(r'\*(.+?)\*', r'\1', text)
        text = re.sub(r'_(.+?)_', r'\1', text)
        # ç§»é™¤æ¨™é¡Œæ¨™è¨˜ #
        text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
        return text

    def rewrite_with_claude(self, original_title: str, original_url: str, sources_data: List[Dict] = None) -> Dict:
        """ä½¿ç”¨ Claude API æ”¹å¯«æ–°è (æ ¹æ“šå‹¾é¸çš„å¤šå€‹ä¾†æºç¶œåˆæ”¹å¯«)"""
        try:
            # å¦‚æœæ²’æœ‰æä¾› sources_dataï¼Œå›å‚³éŒ¯èª¤
            if not sources_data or len(sources_data) == 0:
                return {
                    'success': False,
                    'error': 'è«‹è‡³å°‘å‹¾é¸ä¸€å€‹æ–°èä¾†æº',
                    'original_title': original_title,
                    'original_url': original_url,
                }

            # æŠ“å–æ‰€æœ‰å‹¾é¸ä¾†æºçš„å®Œæ•´å…§å®¹
            all_sources_content = []
            for source_info in sources_data:
                source_name = source_info.get('source', 'æœªçŸ¥ä¾†æº')
                source_title = source_info.get('title', '')
                source_url = source_info.get('url', '')

                try:
                    html = self.crawler.fetch_html(source_url)
                    soup = BeautifulSoup(html, 'html.parser')
                    paragraphs = soup.find_all('p')
                    # æŠ“å–æœ€å¤š 20 æ®µ
                    content_paragraphs = [p.get_text(strip=True) for p in paragraphs[:20] if p.get_text(strip=True)]
                    full_content = '\n\n'.join(content_paragraphs)

                    if full_content:
                        all_sources_content.append({
                            'source': source_name,
                            'title': source_title,
                            'url': source_url,
                            'content': full_content,
                        })
                    else:
                        # å¦‚æœæŠ“å–å¤±æ•—ï¼Œè¨˜éŒ„ä½†ç¹¼çºŒ
                        all_sources_content.append({
                            'source': source_name,
                            'title': source_title,
                            'url': source_url,
                            'content': 'ï¼ˆç„¡æ³•å–å¾—å®Œæ•´å…§å®¹ï¼‰',
                        })
                except Exception as e:
                    print(f"âš ï¸  æŠ“å– {source_name} å…§å®¹å¤±æ•—: {e}")
                    all_sources_content.append({
                        'source': source_name,
                        'title': source_title,
                        'url': source_url,
                        'content': f'ï¼ˆæŠ“å–å¤±æ•—: {str(e)}ï¼‰',
                    })

            # å¦‚æœæ‰€æœ‰ä¾†æºéƒ½æŠ“å–å¤±æ•—ï¼Œå›å‚³éŒ¯èª¤
            valid_sources = [s for s in all_sources_content if 'ï¼ˆç„¡æ³•å–å¾—å®Œæ•´å…§å®¹ï¼‰' not in s['content'] and 'ï¼ˆæŠ“å–å¤±æ•—' not in s['content']]
            if len(valid_sources) == 0:
                return {
                    'success': False,
                    'error': 'æ‰€æœ‰ä¾†æºçš„å…§å®¹éƒ½ç„¡æ³•å–å¾—ï¼Œç„¡æ³•é€²è¡Œæ”¹å¯«',
                    'original_title': original_title,
                    'original_url': original_url,
                }

            # æº–å‚™åŒ…å«å”é®å®‡æŠ€èƒ½çš„ system prompt
            system_prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–°èè¨˜è€…ã€‚è«‹åš´æ ¼éµå¾ªä»¥ä¸‹å”é®å®‡çš„æ–°èå ±å°å¯«ä½œæŠ€èƒ½:

{TCY_SKILL}

ã€æ ¸å¿ƒåŸå‰‡ - çµ•å°åš´æ ¼éµå®ˆã€‘
1. çµ•å°ä¾æ“šç´ ææ’°å¯« - åªèƒ½æ ¹æ“šæä¾›çš„æ–°èä¾†æºå…§å®¹æ”¹å¯«ï¼Œä¸å¾—æ·»åŠ ä»»ä½•åŸæ–‡æ²’æœ‰çš„è³‡è¨Š
2. ç¦æ­¢æ¨æ¸¬æˆ–ç·¨é€  - å¦‚æœåŸæ–‡æ²’æœ‰æåˆ°çš„äº‹æƒ…ï¼Œçµ•å°ä¸è¦å¯«å‡ºä¾†
3. ç¦æ­¢ä½¿ç”¨ä¸å…·é«”çš„æ¶ˆæ¯ä¾†æº - ä¸å¯ä½¿ç”¨ã€Œæ“šäº†è§£ã€ã€ã€Œæ¶ˆæ¯äººå£«è¡¨ç¤ºã€ã€ã€Œæœ‰é—œäººå£«æŒ‡å‡ºã€ã€ã€Œæ“šæ‚‰ã€ç­‰æ¨¡ç³Šä¾†æº
4. å…§å®¹å¿…é ˆå¯è¿½æº¯ - æ”¹å¯«çš„æ¯ä¸€å¥è©±éƒ½å¿…é ˆèƒ½åœ¨æä¾›çš„ä¾†æºä¸­æ‰¾åˆ°å°æ‡‰çš„åŸæ–‡
5. ä¸è¦å»¶ä¼¸æˆ–æ¨è«– - åªæ”¹å¯«æ–‡å­—ï¼Œä¸å¢åŠ ä»»ä½•è§£é‡‹ã€åˆ†ææˆ–å»¶ä¼¸å…§å®¹

ã€å¯«ä½œæŠ€å·§ã€‘
1. å€’é‡‘å­—å¡”çµæ§‹ - æœ€é‡è¦è³‡è¨Šåœ¨å‰ï¼Œä¾é‡è¦æ€§éæ¸›
2. é‡‘å­—å¡”åŸç† - æ¯æ®µé¦–å¥æ˜¯æ ¸å¿ƒè«–é»ï¼Œå¾ŒçºŒå…§å®¹æ”¯æ’è©²è«–é»
3. 5W1H å°è¨€ - ä½•æ™‚ã€ä½•åœ°ã€ä½•äººã€ä½•äº‹ã€ç‚ºä½•ã€å¦‚ä½•
4. æ•¸æ“šå…ˆè¡Œ - ç”¨å…·é«”æ•¸å­—ã€çµ±è¨ˆè³‡æ–™é–‹å ´ï¼ˆå¦‚æœåŸæ–‡æœ‰æä¾›ï¼‰
5. å¤šæ–¹è²éŸ³ - å¦‚æœåŸæ–‡æœ‰å¼•è¿°ä¸åŒäººçš„èªªæ³•ï¼Œè¦ä¿ç•™é€™äº›å¼•è¿°

ã€æ ¼å¼è¦æ±‚ã€‘
- çµ•å°ä¸å¯ä½¿ç”¨ Markdown æ ¼å¼
- ä¸å¯ä½¿ç”¨ **ç²—é«”**ã€*æ–œé«”*ã€# æ¨™é¡Œç­‰ä»»ä½• Markdown èªæ³•
- ä½¿ç”¨ç´”æ–‡å­—è¼¸å‡ºï¼Œä¸éœ€ä»»ä½•æ ¼å¼æ¨™è¨˜
- å¦‚éœ€å¼·èª¿ï¼Œä½¿ç”¨ã€Œã€æˆ–ç›´æ¥åŠ å¼·èªæ°£çš„æ–‡å­—å³å¯

ã€ç¦æ­¢äº‹é … - æ¥µåº¦é‡è¦ã€‘
âŒ ä¸å¯æ·»åŠ åŸæ–‡æ²’æœ‰çš„æ•¸æ“šã€æ™‚é–“ã€åœ°é»ã€äººå
âŒ ä¸å¯ä½¿ç”¨ã€Œæ“šäº†è§£ã€ã€ã€Œæ¶ˆæ¯äººå£«ã€ã€ã€Œæœ‰é—œäººå£«ã€ç­‰ä¸å…·é«”ä¾†æº
âŒ ä¸å¯æ¨æ¸¬äº‹ä»¶çš„åŸå› ã€å½±éŸ¿æˆ–æœªä¾†ç™¼å±•ï¼ˆé™¤éåŸæ–‡æœ‰æ˜ç¢ºæåˆ°ï¼‰
âŒ ä¸å¯ç·¨é€ ä»»ä½•å¼•è¿°æˆ–å°è©±
âŒ ä¸å¯æ·»åŠ ä»»ä½•èƒŒæ™¯è³‡è¨Šï¼ˆé™¤éåŸæ–‡æœ‰æä¾›ï¼‰
"""

            # æ•´ç†æ‰€æœ‰ä¾†æºå…§å®¹
            sources_text = ""
            for idx, source_data in enumerate(all_sources_content, 1):
                sources_text += f"\n{'='*60}\n"
                sources_text += f"ä¾†æº {idx}: {source_data['source']}\n"
                sources_text += f"æ¨™é¡Œ: {source_data['title']}\n"
                sources_text += f"ç¶²å€: {source_data['url']}\n"
                sources_text += f"\nå®Œæ•´å…§å®¹:\n{source_data['content']}\n"

            user_prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ {len(all_sources_content)} å€‹æ–°èä¾†æºçš„å¯¦éš›å…§å®¹ï¼Œç¶œåˆæ”¹å¯«æˆä¸€ç¯‡ ETtoday é¢¨æ ¼çš„å°ˆæ¥­å ±å°ã€‚

{sources_text}

{'='*60}

ã€æ”¹å¯«è¦æ±‚ã€‘
1. æ¨™é¡Œï¼šç°¡æ½”æœ‰åŠ›ï¼Œ50 å­—ä»¥å…§ï¼Œå¿…é ˆåŸºæ–¼ä¸Šè¿°ä¾†æºå…§å®¹
2. å…§æ–‡ï¼šå®Œæ•´å ±å°ï¼Œå»ºè­° 800 å­—ä»¥å…§
   - ä½¿ç”¨å€’é‡‘å­—å¡”çµæ§‹
   - æ¯å€‹è³‡è¨Šéƒ½å¿…é ˆèƒ½åœ¨ä¸Šè¿°ä¾†æºä¸­æ‰¾åˆ°å°æ‡‰å…§å®¹
   - å¦‚æœå¤šå€‹ä¾†æºæœ‰ä¸åŒèªªæ³•ï¼Œå¯ä»¥ç¶œåˆå‘ˆç¾
   - çµ•å°ä¸è¦æ·»åŠ ä¾†æºæ²’æœ‰çš„è³‡è¨Š

ã€é‡è¦æé†’ã€‘
- æ”¹å¯«æ™‚åªèƒ½é‡æ–°çµ„ç¹”å’Œæ½¤é£¾ä¸Šè¿°ä¾†æºçš„å…§å®¹
- ä¸å¯æ·»åŠ ä»»ä½•ä¸Šè¿°ä¾†æºæ²’æœ‰æåˆ°çš„è³‡è¨Š
- ä¸å¯ä½¿ç”¨ã€Œæ“šäº†è§£ã€ã€ã€Œæ¶ˆæ¯äººå£«ã€ç­‰ä¸å…·é«”ä¾†æº
- æ¯å¥è©±éƒ½å¿…é ˆæœ‰æ˜ç¢ºçš„ä¾†æºä¾æ“š

è«‹ç›´æ¥æä¾›æ”¹å¯«å¾Œçš„æ¨™é¡Œå’Œå…§æ–‡ï¼Œä¸éœ€è¦ä»»ä½•èªªæ˜æ–‡å­—ã€‚

æ ¼å¼å¦‚ä¸‹ï¼š
æ¨™é¡Œï¼šï¼ˆæ”¹å¯«å¾Œçš„æ¨™é¡Œï¼‰

å…§æ–‡ï¼šï¼ˆå®Œæ•´å…§æ–‡ï¼‰
"""

            # å‘¼å« Claude API
            message = self.claude.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=2000,
                temperature=0.3,  # é™ä½ temperature æ¸›å°‘å‰µé€ æ€§ï¼Œå¢åŠ äº‹å¯¦æ€§
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )

            # è§£æå›æ‡‰
            response_text = message.content[0].text.strip()

            # è§£ææ¨™é¡Œå’Œå…§æ–‡
            title = ""
            body = ""

            if "æ¨™é¡Œï¼š" in response_text or "æ¨™é¡Œ:" in response_text:
                # æå–æ¨™é¡Œ
                title_match = response_text.split("æ¨™é¡Œï¼š" if "æ¨™é¡Œï¼š" in response_text else "æ¨™é¡Œ:")[1].split("\n")[0].strip()
                title = title_match

                # æå–å…§æ–‡
                if "å…§æ–‡ï¼š" in response_text:
                    body = response_text.split("å…§æ–‡ï¼š")[1].strip()
                elif "å…§æ–‡:" in response_text:
                    body = response_text.split("å…§æ–‡:")[1].strip()
            else:
                # å¦‚æœæ²’æœ‰æ˜ç¢ºæ¨™è¨˜ï¼Œä½¿ç”¨åŸæ¨™é¡Œ
                title = original_title
                body = response_text

            # æ¸…ç† Markdown æ ¼å¼
            clean_title = self.clean_markdown(title)
            clean_body = self.clean_markdown(body)

            return {
                'success': True,
                'title': clean_title,
                'body': clean_body,
                'original_title': original_title,
                'original_url': original_url,
                'sources_used': [s['source'] for s in all_sources_content],
                'model': 'claude-sonnet-4-20250514',
                'method': 'multi_source_rewrite',
            }

        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'original_title': original_title,
                'original_url': original_url,
            }


# å»ºç«‹å…¨åŸŸå¯¦ä¾‹
dashboard = NewsDashboard()


@app.route('/')
def index():
    """é¦–é """
    return render_template('dashboard.html')


@app.route('/api/crawl', methods=['POST'])
def api_crawl():
    """çˆ¬å–æ‰€æœ‰ä¾†æºï¼ˆå¹³è¡ŒåŸ·è¡Œï¼‰"""
    import time
    start_time = time.time()
    print(f"\n{'='*60}", flush=True)
    print(f"ğŸš€ é–‹å§‹åˆ†ææµç¨‹ (æ™‚é–“æˆ³: {time.strftime('%Y-%m-%d %H:%M:%S')})", flush=True)
    print(f"{'='*60}", flush=True)

    try:
        # å®šç¾©çˆ¬å–ä»»å‹™
        def crawl_udn():
            config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'udn'), None)
            return ('UDN', dashboard.crawl_source(config) if config else [])

        def crawl_tvbs():
            config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'tvbs'), None)
            return ('TVBS', dashboard.crawl_source(config) if config else [])

        def crawl_chinatimes():
            config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'chinatimes'), None)
            return ('ä¸­æ™‚æ–°èç¶²', dashboard.crawl_source(config) if config else [])

        def crawl_setn():
            config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'setn'), None)
            return ('ä¸‰ç«‹æ–°èç¶²', dashboard.crawl_source(config) if config else [])

        def crawl_et():
            return ('ETtoday', dashboard.crawl_ettoday())

        # å¹³è¡Œçˆ¬å–æ‰€æœ‰ä¾†æºï¼ˆæœ€å¤š 2 å€‹åŒæ™‚åŸ·è¡Œï¼Œæ¸›å°‘è¨˜æ†¶é«”å£“åŠ›ï¼‰
        results = {}
        with ThreadPoolExecutor(max_workers=2) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            futures = [
                executor.submit(crawl_udn),
                executor.submit(crawl_tvbs),
                executor.submit(crawl_chinatimes),
                executor.submit(crawl_setn),
                executor.submit(crawl_et),
            ]

            # æ”¶é›†çµæœ
            for future in as_completed(futures):
                source_name, items = future.result()
                results[source_name] = items

        # ä¸»å‹•å›æ”¶è¨˜æ†¶é«”ï¼ˆé‡‹æ”¾ BeautifulSoup è§£æç”¢ç”Ÿçš„å¤§é‡ç‰©ä»¶ï¼‰
        import gc
        gc.collect()

        # æå–çµæœ
        udn_items = results.get('UDN', [])
        tvbs_items = results.get('TVBS', [])
        chinatimes_items = results.get('ä¸­æ™‚æ–°èç¶²', [])
        setn_items = results.get('ä¸‰ç«‹æ–°èç¶²', [])
        ettoday_items = results.get('ETtoday', [])

        # çµ„åˆæ‰€æœ‰ä¾†æºçš„å­—å…¸
        all_source_items = {
            'UDN': udn_items,
            'TVBS': tvbs_items,
            'ä¸­æ™‚æ–°èç¶²': chinatimes_items,
            'ä¸‰ç«‹æ–°èç¶²': setn_items,
        }

        # æ‰¾å‡º ETtoday ç¼ºå°‘çš„æ–°èï¼ˆä½¿ç”¨æ··åˆç›¸ä¼¼åº¦ç­–ç•¥ï¼‰
        print(f"\nâ±ï¸  éšæ®µ 1 å®Œæˆï¼ˆçˆ¬å–ï¼‰: {time.time() - start_time:.2f} ç§’")
        print(f"ğŸ“Š çˆ¬å–çµæœçµ±è¨ˆ:")
        for source, items in all_source_items.items():
            print(f"   - {source}: {len(items)} å‰‡")
        print(f"   - ETtoday: {len(ettoday_items)} å‰‡")

        print(f"\nğŸ” éšæ®µ 2 é–‹å§‹ï¼ˆç›¸ä¼¼åº¦æ¯”å°ï¼‰...")
        stage2_start = time.time()

        missing_news = dashboard.find_missing_news(all_source_items, ettoday_items)

        print(f"â±ï¸  éšæ®µ 2 å®Œæˆï¼ˆæ¯”å°ï¼‰: {time.time() - stage2_start:.2f} ç§’")

        # å–å¾— LLM èª¿ç”¨æ¬¡æ•¸çµ±è¨ˆ
        llm_calls = dashboard.similarity_checker.llm_call_count
        print(f"ğŸ“Š LLM èª¿ç”¨çµ±è¨ˆ: {llm_calls} æ¬¡")

        total_time = time.time() - start_time
        print(f"\nâœ… åˆ†æå®Œæˆï¼ç¸½è€—æ™‚: {total_time:.2f} ç§’", flush=True)
        print(f"   - æ‰¾åˆ°ç¼ºå°‘æ–°è: {len(missing_news)} å‰‡", flush=True)
        print(f"{'='*60}\n", flush=True)

        return jsonify({
            'success': True,
            'udn': [{'source': i.source, 'title': i.title, 'url': i.url} for i in udn_items],
            'tvbs': [{'source': i.source, 'title': i.title, 'url': i.url} for i in tvbs_items],
            'ä¸­æ™‚æ–°èç¶²': [{'source': i.source, 'title': i.title, 'url': i.url} for i in chinatimes_items],
            'ä¸‰ç«‹æ–°èç¶²': [{'source': i.source, 'title': i.title, 'url': i.url} for i in setn_items],
            'ettoday': [{'source': i.source, 'title': i.title, 'url': i.url} for i in ettoday_items],
            'missing': missing_news,
            'llm_calls': llm_calls,
            'total_time': f"{total_time:.2f}s",
        })

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"\nâŒ éŒ¯èª¤ç™¼ç”Ÿ:")
        print(error_detail)
        print(f"{'='*60}\n")
        return jsonify({
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__,
        }), 500


@app.route('/api/rewrite', methods=['POST'])
def api_rewrite():
    """æ”¹å¯«å–®å‰‡æ–°èï¼ˆæ ¹æ“šå‹¾é¸çš„å¤šå€‹ä¾†æºï¼‰"""
    try:
        data = request.json
        title = data.get('title', '')
        url = data.get('url', '')
        sources = data.get('sources', [])  # å‹¾é¸çš„ä¾†æºåˆ—è¡¨

        if not title:
            return jsonify({'success': False, 'error': 'Title is required'}), 400

        if not sources or len(sources) == 0:
            return jsonify({'success': False, 'error': 'è«‹è‡³å°‘å‹¾é¸ä¸€å€‹æ–°èä¾†æº'}), 400

        # å‘¼å«æ”¹å¯«å‡½æ•¸ï¼Œå‚³å…¥å‹¾é¸çš„ä¾†æº
        result = dashboard.rewrite_with_claude(title, url, sources)
        return jsonify(result)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


if __name__ == '__main__':
    os.makedirs('templates', exist_ok=True)

    print("=" * 60)
    print("ğŸš€ æ–°èç›£æ§å„€è¡¨æ¿ v1.1 (Prototype) å•Ÿå‹•ä¸­...")
    print("ğŸ“ è¨ªå•: http://localhost:8080")
    print("=" * 60)
    print("ğŸ’¡ ä½¿ç”¨æ–¹å¼: System Prompt + å”é®å®‡å¯«ä½œæŠ€èƒ½")
    print("   (Skills API åœ¨ Python SDK ä¸­å°šæœªå®Œå…¨æ”¯æ´)")
    print("=" * 60)

    app.run(debug=True, host='0.0.0.0', port=8080)
