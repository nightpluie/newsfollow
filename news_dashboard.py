#!/usr/bin/env python3
"""
æ–°èç›£æ§å„€è¡¨æ¿ - æ¯”å° ETtoday ä¸¦ç”¨ Claude æ”¹å¯«
ä½¿ç”¨ Claude API (å« Skill æ”¯æ´çš„ fallback ç‰ˆæœ¬)
"""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from typing import Dict, List

import anthropic
import requests
import yaml
from bs4 import BeautifulSoup
from flask import Flask, jsonify, render_template, request
from flask_cors import CORS

# è¼‰å…¥åŸæœ‰çš„å‡½æ•¸
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from main import (
    RequestsCrawler,
    Signal,
    extract_signals,
    normalize_title,
    now_iso,
)
from hybrid_similarity import HybridSimilarityChecker

app = Flask(__name__)
CORS(app)

# Claude API è¨­å®š (å¾ç’°å¢ƒè®Šæ•¸è®€å–)
CLAUDE_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")

# è¼‰å…¥å¯«ä½œæŠ€èƒ½ (å¯é¸)
SKILL_PATH = os.getenv("SKILL_PATH", "./SKILL.md")
TCY_SKILL = ""
if os.path.exists(SKILL_PATH):
    try:
        with open(SKILL_PATH, 'r', encoding='utf-8') as f:
            TCY_SKILL = f.read()
        print(f"âœ… å·²è¼‰å…¥å¯«ä½œæŠ€èƒ½: {SKILL_PATH}")
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•è¼‰å…¥æŠ€èƒ½æª”æ¡ˆ: {e}")


@dataclass
class NewsItem:
    """æ–°èé …ç›®"""
    source: str
    title: str
    url: str
    normalized_title: str
    crawled_at: str


class NewsDashboard:
    def __init__(self, config_path: str = "./config.yaml"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.crawler = RequestsCrawler()
        if not CLAUDE_API_KEY:
            print("âš ï¸  æœªè¨­å®š ANTHROPIC_API_KEY ç’°å¢ƒè®Šæ•¸ï¼Œæ”¹å¯«åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
        self.claude = anthropic.Anthropic(api_key=CLAUDE_API_KEY) if CLAUDE_API_KEY else None

        # åˆå§‹åŒ–æ··åˆç›¸ä¼¼åº¦æª¢æŸ¥å™¨ï¼ˆæ¼”ç®—æ³• + LLMï¼‰
        self.similarity_checker = HybridSimilarityChecker(enable_llm=True)
        print("âœ… æ··åˆç›¸ä¼¼åº¦æª¢æŸ¥å™¨å·²å•Ÿç”¨ï¼ˆæ¼”ç®—æ³• + GPT-4o-miniï¼‰")

    def crawl_source(self, source_config: Dict) -> List[NewsItem]:
        """çˆ¬å–å–®ä¸€åª’é«”ä¾†æº"""
        items = []

        for section in source_config['sections']:
            try:
                html = self.crawler.fetch_html(section['url'])
                signals = extract_signals(
                    html=html,
                    base_url=section['url'],
                    source_id=source_config['source_id'],
                    source_name=source_config['source_name'],
                    section_id=section['section_id'],
                    domain_contains=source_config.get('domain_contains', ''),
                    selectors=section.get('selectors', []),
                    weight=section.get('weight', 1),
                    crawled_at=now_iso(),
                    max_items=section.get('max_items', 20),
                )

                for sig in signals:
                    items.append(NewsItem(
                        source=sig.source_name,
                        title=sig.title,
                        url=sig.url,
                        normalized_title=sig.normalized_title,
                        crawled_at=sig.crawled_at,
                    ))

            except Exception as e:
                print(f"Error crawling {source_config['source_id']}/{section['section_id']}: {e}")

        return items

    def crawl_ettoday(self) -> List[NewsItem]:
        """çˆ¬å– ETtoday æ–°è"""
        items = []

        urls = [
            "https://www.ettoday.net/news/news-list.htm",
            "https://www.ettoday.net/news/focus/focus-list.htm",
        ]

        for url in urls:
            try:
                html = self.crawler.fetch_html(url)
                soup = BeautifulSoup(html, 'html.parser')

                selectors = [
                    "h3 a",
                    ".part_list_2 h3 a",
                    ".piece h3 a",
                ]

                for selector in selectors:
                    for link in soup.select(selector):
                        title = link.get_text(strip=True)
                        href = link.get('href', '')

                        if not title or len(title) < 8:
                            continue

                        full_url = href if href.startswith('http') else f"https://www.ettoday.net{href}"

                        items.append(NewsItem(
                            source="ETtoday",
                            title=title,
                            url=full_url,
                            normalized_title=normalize_title(title),
                            crawled_at=now_iso(),
                        ))

            except Exception as e:
                print(f"Error crawling ETtoday {url}: {e}")

        # å»é‡
        seen = set()
        unique_items = []
        for item in items:
            key = item.normalized_title
            if key not in seen:
                seen.add(key)
                unique_items.append(item)

        return unique_items

    def find_missing_news(self, udn_items: List[NewsItem], tvbs_items: List[NewsItem],
                         ettoday_items: List[NewsItem]) -> List[Dict]:
        """
        æ‰¾å‡º ETtoday æ²’æœ‰çš„æ–°è
        ä½¿ç”¨æ··åˆç­–ç•¥ï¼ˆæ¼”ç®—æ³• + LLMï¼‰é€²è¡Œç›¸ä¼¼åº¦æ¯”å°
        """
        # æ”¶é›† ETtoday æ‰€æœ‰æ¨™é¡Œï¼ˆç”¨æ–¼æ··åˆæ¯”å°ï¼‰
        ettoday_titles_list = [item.title for item in ettoday_items]
        all_items = udn_items + tvbs_items

        # é‡ç½®çµ±è¨ˆè³‡è¨Š
        self.similarity_checker.reset_statistics()

        missing = []
        for item in all_items:
            # ä½¿ç”¨æ··åˆç­–ç•¥æª¢æŸ¥æ˜¯å¦åœ¨ ETtoday ä¸­å­˜åœ¨
            is_in_ettoday = self.similarity_checker.batch_check(
                candidate_title=item.title,
                reference_titles=ettoday_titles_list
            )

            # åªæœ‰ç•¶ç¢ºå®šä¸åœ¨ ETtoday æ™‚ï¼Œæ‰åŠ å…¥ç¼ºå°‘åˆ—è¡¨
            if not is_in_ettoday:
                # é¿å…é‡è¤‡ï¼ˆæª¢æŸ¥æ˜¯å¦å·²åœ¨ missing åˆ—è¡¨ä¸­ï¼‰
                if not any(m['normalized_title'] == item.normalized_title for m in missing):
                    missing.append({
                        'source': item.source,
                        'title': item.title,
                        'url': item.url,
                        'normalized_title': item.normalized_title,
                        'crawled_at': item.crawled_at,
                    })

        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = self.similarity_checker.get_statistics()
        print(f"ğŸ“Š ç›¸ä¼¼åº¦æ¯”å°çµ±è¨ˆ: LLM èª¿ç”¨æ¬¡æ•¸ = {stats['llm_call_count']}")

        return missing

    def rewrite_with_claude(self, original_title: str, original_url: str) -> Dict:
        """ä½¿ç”¨ Claude API æ”¹å¯«æ–°è (ä½¿ç”¨å”é®å®‡æŠ€èƒ½æŒ‡å¼•)"""
        try:
            # å…ˆå˜—è©¦æŠ“å–åŸæ–‡å…§å®¹
            try:
                html = self.crawler.fetch_html(original_url)
                soup = BeautifulSoup(html, 'html.parser')
                paragraphs = soup.find_all('p')
                original_content = '\n'.join([p.get_text(strip=True) for p in paragraphs[:10]])
            except:
                original_content = ""

            # æº–å‚™åŒ…å«å”é®å®‡æŠ€èƒ½çš„ system prompt
            system_prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–°èè¨˜è€…ã€‚è«‹åš´æ ¼éµå¾ªä»¥ä¸‹å”é®å®‡çš„æ–°èå ±å°å¯«ä½œæŠ€èƒ½:

{TCY_SKILL}

é‡è¦æé†’:
1. **åš´æ ¼ä¾æ“šç´ ææ’°å¯«** - åªèƒ½æ ¹æ“šæä¾›çš„ç´ æ,ç¦æ­¢è‡ªè¡Œæ£æƒ³æˆ–ç·¨é€ 
2. **å€’é‡‘å­—å¡”çµæ§‹** - æœ€é‡è¦è³‡è¨Šåœ¨å‰,ä¾é‡è¦æ€§éæ¸›
3. **é‡‘å­—å¡”åŸç†** - æ¯æ®µé¦–å¥æ˜¯æ ¸å¿ƒè«–é»,å¾ŒçºŒå…§å®¹æ”¯æ’è©²è«–é»
4. **å°è¨€æ¶µè“‹ 5W1H** - ä½•æ™‚ã€ä½•åœ°ã€ä½•äººã€ä½•äº‹ã€ç‚ºä½•ã€å¦‚ä½•
5. **æ•¸æ“šå…ˆè¡Œ** - ç”¨å…·é«”æ•¸å­—ã€çµ±è¨ˆè³‡æ–™é–‹å ´
6. **250å­—å°è¨€** - ç²¾ç…‰æ ¸å¿ƒé‡é»,ä¸è¶…é250å­—
"""

            user_prompt = f"""è«‹å°‡ä»¥ä¸‹æ–°èæ”¹å¯«æˆ ETtoday é¢¨æ ¼çš„å°ˆæ¥­å ±å°:

**åŸå§‹æ¨™é¡Œ:** {original_title}
**åŸå§‹ä¾†æº:** {original_url}

**åŸå§‹å…§å®¹:**
{original_content if original_content else "ï¼ˆç„¡æ³•å–å¾—å®Œæ•´å…§å®¹,è«‹æ ¹æ“šæ¨™é¡Œæ¨æ¸¬ä¸¦æ”¹å¯«,ä½†è«‹æ˜ç¢ºæ¨™ç¤ºç‚ºæ¨æ¸¬å…§å®¹ï¼‰"}

è«‹æä¾›:
1. æ–°èæ¨™é¡Œ (ç°¡æ½”æœ‰åŠ›,50å­—ä»¥å…§)
2. å°è¨€ (250å­—ä»¥å…§,æ¶µè“‹ 5W1H,æ•¸æ“šå…ˆè¡Œ)
3. å®Œæ•´å…§æ–‡ (ä¾å€’é‡‘å­—å¡”çµæ§‹,ç´„ 400-600 å­—)

è«‹ä»¥ JSON æ ¼å¼å›å‚³:
{{
    "title": "æ”¹å¯«å¾Œçš„æ¨™é¡Œ",
    "lead": "å°è¨€å…§å®¹ (250å­—å…§)",
    "body": "å®Œæ•´å…§æ–‡ (400-600å­—)"
}}
"""

            # å‘¼å« Claude API
            message = self.claude.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=2000,
                temperature=0.7,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )

            # è§£æå›æ‡‰
            response_text = message.content[0].text

            # å˜—è©¦æå– JSON
            try:
                if "```json" in response_text:
                    response_text = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    response_text = response_text.split("```")[1].split("```")[0]

                result = json.loads(response_text.strip())

                return {
                    'success': True,
                    'title': result.get('title', ''),
                    'lead': result.get('lead', ''),
                    'body': result.get('body', ''),
                    'original_title': original_title,
                    'original_url': original_url,
                    'model': 'claude-sonnet-4-20250514',
                    'method': 'system_prompt_with_skill',
                }

            except json.JSONDecodeError:
                return {
                    'success': True,
                    'title': original_title,
                    'lead': response_text[:250],
                    'body': response_text,
                    'original_title': original_title,
                    'original_url': original_url,
                    'model': 'claude-sonnet-4-20250514',
                    'method': 'system_prompt_with_skill',
                    'note': 'Failed to parse JSON, returning raw text',
                }

        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'original_title': original_title,
                'original_url': original_url,
            }


# å»ºç«‹å…¨åŸŸå¯¦ä¾‹
dashboard = NewsDashboard()


@app.route('/')
def index():
    """é¦–é """
    return render_template('dashboard.html')


@app.route('/api/crawl', methods=['POST'])
def api_crawl():
    """çˆ¬å–æ‰€æœ‰ä¾†æº"""
    try:
        udn_config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'udn'), None)
        udn_items = dashboard.crawl_source(udn_config) if udn_config else []

        tvbs_config = next((s for s in dashboard.config['sources'] if s['source_id'] == 'tvbs'), None)
        tvbs_items = dashboard.crawl_source(tvbs_config) if tvbs_config else []

        ettoday_items = dashboard.crawl_ettoday()

        missing_news = dashboard.find_missing_news(udn_items, tvbs_items, ettoday_items)

        return jsonify({
            'success': True,
            'udn': [{'source': i.source, 'title': i.title, 'url': i.url} for i in udn_items],
            'tvbs': [{'source': i.source, 'title': i.title, 'url': i.url} for i in tvbs_items],
            'ettoday': [{'source': i.source, 'title': i.title, 'url': i.url} for i in ettoday_items],
            'missing': missing_news,
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/rewrite', methods=['POST'])
def api_rewrite():
    """æ”¹å¯«å–®å‰‡æ–°è"""
    try:
        data = request.json
        title = data.get('title', '')
        url = data.get('url', '')

        if not title:
            return jsonify({'success': False, 'error': 'Title is required'}), 400

        result = dashboard.rewrite_with_claude(title, url)
        return jsonify(result)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


if __name__ == '__main__':
    os.makedirs('templates', exist_ok=True)

    print("=" * 60)
    print("ğŸš€ æ–°èç›£æ§å„€è¡¨æ¿å•Ÿå‹•ä¸­...")
    print("ğŸ“ è¨ªå•: http://localhost:8080")
    print("=" * 60)
    print("ğŸ’¡ ä½¿ç”¨æ–¹å¼: System Prompt + å”é®å®‡å¯«ä½œæŠ€èƒ½")
    print("   (Skills API åœ¨ Python SDK ä¸­å°šæœªå®Œå…¨æ”¯æ´)")
    print("=" * 60)

    app.run(debug=True, host='0.0.0.0', port=8080)
